{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denis04-M/phishing-lab/blob/main/phishing_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUMK-FYfWNuq",
        "outputId": "796d3daf-ba9a-4825-8f65-aff38ce85a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin:__________________________________\n",
            "########################Logistic Regression##############################\n",
            "Accuracy: 1.00\n",
            "confusion matrix\n",
            "[[2464    0]\n",
            " [   0 3064]]\n",
            "Predicted     0     1   All\n",
            "True                       \n",
            "0          2464     0  2464\n",
            "1             0  3064  3064\n",
            "All        2464  3064  5528\n",
            "Precision: 1.000\n",
            "Recall: 1.000\n",
            "F1-measure: 1.000\n",
            "########################Random Forest##############################\n",
            "Accuracy: 1.00\n",
            "confusion matrix\n",
            "[[2464    0]\n",
            " [   0 3064]]\n",
            "Predicted     0     1   All\n",
            "True                       \n",
            "0          2464     0  2464\n",
            "1             0  3064  3064\n",
            "All        2464  3064  5528\n",
            "Precision: 1.000\n",
            "Recall: 1.000\n",
            "F1-measure: 1.000\n",
            "#######################Decision Tree#######################\n",
            "Accuracy: 1.00\n",
            "confusion matrix\n",
            "[[2464    0]\n",
            " [   0 3064]]\n",
            "Predicted   0.0   1.0   All\n",
            "True                       \n",
            "0          2464     0  2464\n",
            "1             0  3064  3064\n",
            "All        2464  3064  5528\n",
            "Precision: 1.000\n",
            "Recall: 1.000\n",
            "F1-measure: 1.000\n",
            "#######################Naive Bayes#######################\n",
            "Accuracy: 1.00\n",
            "confusion matrix\n",
            "[[2464    0]\n",
            " [   0 3064]]\n",
            "Predicted     0     1   All\n",
            "True                       \n",
            "0          2464     0  2464\n",
            "1             0  3064  3064\n",
            "All        2464  3064  5528\n",
            "Precision: 1.000\n",
            "Recall: 1.000\n",
            "F1-measure: 1.000\n"
          ]
        }
      ],
      "source": [
        "from warnings import simplefilter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from numpy import genfromtxt\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "#from sklearn.datasets import fetch_mldata\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score,\n",
        "                             precision_score, recall_score)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "feature=genfromtxt('phishing.csv',delimiter=',',usecols=(i for i in range(0,31)),skip_header=1)\n",
        "target=genfromtxt('phishing.csv',delimiter=',',usecols=(-1),skip_header=1)\n",
        "sc = StandardScaler()\n",
        "sc.fit(feature)\n",
        "target_label = LabelEncoder().fit_transform(target)\n",
        "feature_std = sc.transform(feature)\n",
        "x_train, x_test, y_train, y_test = train_test_split(feature_std, target_label, test_size=0.5, random_state=1)\n",
        "\n",
        "print(\"Begin:__________________________________\")\n",
        "###################################################\n",
        "## print stats \n",
        "precision_scores_list = []\n",
        "accuracy_scores_list = []\n",
        "\n",
        "def print_stats_metrics(y_test, y_pred):    \n",
        "    print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )\n",
        "    #Accuracy: 0.84\n",
        "    accuracy_scores_list.append(accuracy_score(y_test,   y_pred) )\n",
        "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "    print (\"confusion matrix\")\n",
        "    print(confmat)\n",
        "    print (pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
        "    precision_scores_list.append(precision_score(y_true=y_test, y_pred=y_pred))\n",
        "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
        "\n",
        "\n",
        "########################Logistic Regression##############################\n",
        "print(\"########################Logistic Regression##############################\")\n",
        "clfLog = LogisticRegression()\n",
        "clfLog.fit(x_train,y_train)\n",
        "predictions = clfLog.predict(x_test)\n",
        "print_stats_metrics(y_test, predictions)\n",
        "\n",
        "########################Random Forest##############################\n",
        "print(\"########################Random Forest##############################\")\n",
        "clfRandForest = RandomForestClassifier()\n",
        "clfRandForest.fit(x_train,y_train)\n",
        "predictions = clfRandForest.predict(x_test)\n",
        "print_stats_metrics(y_test, predictions)\n",
        "#######################Decision Tree#######################\n",
        "print(\"#######################Decision Tree#######################\")\n",
        "clfDT = DecisionTreeRegressor()\n",
        "clfDT.fit(x_train,y_train)\n",
        "predictions = clfDT.predict(x_test)\n",
        "print_stats_metrics(y_test, predictions)\n",
        "#######################Naive Bayes#######################\n",
        "print(\"#######################Naive Bayes#######################\")\n",
        "clfNB = GaussianNB()\n",
        "clfNB.fit(x_train,y_train)\n",
        "predictions = clfNB.predict(x_test)\n",
        "print_stats_metrics(y_test, predictions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0K8VWm2i1Bc",
        "outputId": "ed7fb48c-e3bb-46e0-a4ad-ad657c2de168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall_score: 1.00\n",
            "Precision_score: 0.56\n",
            "F1_score: 0.72\n"
          ]
        }
      ],
      "source": [
        "print('Recall_score: %.2f' % recall_score(y_test, predictions) )\n",
        "print('Precision_score: %.2f' % precision_score(y_test, predictions) )\n",
        "print('F1_score: %.2f' % f1_score(y_test, predictions) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import genfromtxt\n",
        "from sklearn import datasets\n",
        "#from sklearn.datasets import fetch_mldata\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###############################################################\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_epochs = 200\n",
        "batch_size = 100\n",
        "\n",
        "def convertOneHot(data):\n",
        "    y_onehot=[0]*len(data)\n",
        "    for i,j in enumerate(data):\n",
        "        y_onehot[i]=[0]*(data.max()+1)\n",
        "        y_onehot[i][j]=1\n",
        "    return y_onehot\n",
        "\n",
        "###############################################################\n",
        "feature=genfromtxt('phishing.csv',delimiter=',',usecols=(i for i in range(0,31)),skip_header=1)\n",
        "target=genfromtxt('phishing.csv',delimiter=',',usecols=(-1),skip_header=1)\n",
        "sc = StandardScaler()\n",
        "sc.fit(feature)\n",
        "target_label = LabelEncoder().fit_transform(target)\n",
        "target_onehot = convertOneHot(target_label)\n",
        "feature_std = sc.transform(feature)\n",
        "x_train, x_test, y_train_onehot, y_test_onehot = train_test_split(feature_std, target_onehot, test_size=0.20, random_state=0)\n",
        "A=x_train.shape[1]\n",
        "B=len(y_train_onehot[0])\n",
        "print(len(y_test_onehot[0]))\n",
        "print(B)\n",
        "print(\"Begin:__________________________________\")\n",
        "#####################################################################\n",
        "\n",
        "def plot_metric_per_epoch():\n",
        "    x_epochs = []\n",
        "    y_epochs = [] \n",
        "    for i, val in enumerate(accuracy_scores_list):\n",
        "        x_epochs.append(i)\n",
        "        y_epochs.append(val)\n",
        "    \n",
        "    plt.scatter(x_epochs, y_epochs,s=50,c='lightgreen', marker='s', label='score')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('score')\n",
        "    plt.title('Score per epoch')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "###################################################\n",
        "## print stats \n",
        "precision_scores_list = []\n",
        "accuracy_scores_list = []\n",
        "\n",
        "def print_stats_metrics(y_test, y_pred):    \n",
        "    print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )\n",
        "    #Accuracy: 0.84\n",
        "    accuracy_scores_list.append(accuracy_score(y_test,   y_pred) )\n",
        "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "    print (\"confusion matrix\")\n",
        "    print(confmat)\n",
        "    print (pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
        "    precision_scores_list.append(precision_score(y_true=y_test, y_pred=y_pred))\n",
        "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
        "\n",
        "\n",
        "###############################################################\n",
        "def layer(input, weight_shape, bias_shape):\n",
        "    weight_stddev = (2.0/weight_shape[0])**0.5\n",
        "    w_init = tf.random_normal_initializer(stddev=weight_stddev)\n",
        "    bias_init = tf.constant_initializer(value=0)\n",
        "    W = tf.get_variable(\"W\", weight_shape, initializer=w_init)\n",
        "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
        "    return tf.nn.relu(tf.matmul(input, W) + b)\n",
        "###############################################################\n",
        "def inference_deep_layers(x_tf, A, B):\n",
        "    with tf.variable_scope(\"hidden_1\"):\n",
        "        hidden_1 = layer(x_tf, [A, 15],[15])\n",
        "    with tf.variable_scope(\"hidden_2\"):\n",
        "        hidden_2 = layer(hidden_1, [15, 5],[5])\n",
        "    with tf.variable_scope(\"output\"):\n",
        "        output = layer(hidden_2, [5, B], [B])\n",
        "    return output\n",
        "###############################################################\n",
        "def loss_deep(output, y_tf):\n",
        "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_tf)\n",
        "    loss = tf.reduce_mean(xentropy) \n",
        "    return loss\n",
        "###########################################################\n",
        "\n",
        "def training(cost):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    train_op = optimizer.minimize(cost)\n",
        "    return train_op\n",
        "\n",
        "###########################################################\n",
        "def evaluate(output, y_tf):\n",
        "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_tf,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    return accuracy\n",
        "###############################################################\n",
        "\n",
        "x_tf = tf.placeholder(\"float\",[None,A])\n",
        "y_tf = tf.placeholder(\"float\",[None,B])\n",
        "###############################################################\n",
        "output = inference_deep_layers(x_tf,A,B)\n",
        "cost = loss_deep(output,y_tf)\n",
        "train_op=training(cost)\n",
        "eval_op=evaluate(output,y_tf)\n",
        "###############################################################\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "###############################################################\n",
        "y_p_metrics = tf.argmax(output,1)\n",
        "###############################################################\n",
        "num_samples_train_set=x_train.shape[0]\n",
        "num_batches = int(num_samples_train_set/batch_size)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    print(\"epoch %s out of %s\"%(i,n_epochs))\n",
        "    for batch_n in range(num_batches):\n",
        "        sta = batch_n*batch_size\n",
        "        end = sta+batch_size\n",
        "        sess.run(train_op,feed_dict={x_tf:x_train[sta:end],y_tf:y_train_onehot[sta:end]})\n",
        "    print (\"-------------------------------------------------------------------------------\")    \n",
        "    print (\"Accuracy score\")\n",
        "    #result = sess.run(eval_op,feed_dict={x_tf:x_test,y_tf:y_test_onehot})\n",
        "    result, y_result_metrics = sess.run([eval_op, y_p_metrics], feed_dict={x_tf: x_test, y_tf: y_test_onehot})\n",
        "    print(\"Run {},{}\".format(i,result))\n",
        "    y_true = np.argmax(y_test_onehot,1)\n",
        "    print_stats_metrics(y_true, y_result_metrics)\n",
        "    if i==n_epochs-1:\n",
        "        plot_metric_per_epoch()\n"
      ],
      "metadata": {
        "id": "hZTJ9TBpFlGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "      neighbors.append(node)\n"
      ],
      "metadata": {
        "id": "DL-IRz_iguaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# 1\n",
        "def perSentence():\n",
        "  word_count = 0\n",
        "\n",
        "  # open file\n",
        "  with open('text.txt') as text_file:\n",
        "    text = text_file.read()\n",
        "\n",
        "  # spliting the text into sentences\n",
        "  sentences = text.split('.\\n')\n",
        "  sentence_count = len(sentences)\n",
        "\n",
        "  for sentence in sentences:\n",
        "\n",
        "      # removing punctuations\n",
        "      lines = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "      # spliting sentences into words\n",
        "      words = lines.split()\n",
        "\n",
        "      word_count += len(words)\n",
        "\n",
        "  print(f'Number of sentences = {sentence_count}')\n",
        "  print(f'Number of words = {word_count}')\n",
        "  print('Number of words per sentence = ', (word_count//sentence_count))\n",
        "\n",
        "  return (word_count//sentence_count)\n",
        "\n",
        "\n",
        "# 2\n",
        "def uniqueWords():\n",
        "\n",
        "  unique_words = set()\n",
        "\n",
        "  # open file\n",
        "  with open('unique.txt') as text_file:\n",
        "    text = text_file.read()\n",
        "\n",
        "  # removing punctuations\n",
        "  words = lines = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "  for word in words:\n",
        "    # add words to set\n",
        "    unique_words.add(word)\n",
        "\n",
        "  print(f'\\n The unique words are:')\n",
        "  for u_word in unique_words:\n",
        "    print(' *', u_word)\n",
        "\n",
        "  return unique_words\n",
        "\n",
        "\n",
        "\n",
        "# 3\n",
        "\n",
        "def wordFrequency():\n",
        "\n",
        "  word_freq = {}\n",
        "  unique_words = set()\n",
        "\n",
        "  with open('unique.txt') as text_file:\n",
        "    text = text_file.read()\n",
        "\n",
        "  # removing punctuations\n",
        "  words = lines = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "  # add words to set\n",
        "  for word in words:\n",
        "    unique_words.add(word)\n",
        "\n",
        "  for u_word in unique_words:\n",
        "    # count word frequency\n",
        "    n = words.count(u_word)\n",
        "    # add word and word frequency to dict.\n",
        "    word_freq.update({u_word:n})\n",
        "\n",
        "  print('\\n', word_freq)\n",
        "\n",
        "  return word_freq\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  perSentence()\n",
        "  uniqueWords()\n",
        "  wordFrequency()\n"
      ],
      "metadata": {
        "id": "Rup6l-tVn-HA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1c70a8-030d-4c1c-a6da-54aa7c512042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences = 11\n",
            "Number of words = 260\n",
            "Number of words per sentence =  23\n",
            "\n",
            " The unique words are:\n",
            " * interpreted\n",
            " * language\n",
            " * unique\n",
            " * therefore\n",
            " * maybe\n",
            " * is\n",
            " * an\n",
            " * programming\n",
            " * stack\n",
            " * a\n",
            " * generalpurpose\n",
            " * Python\n",
            " * highlevel\n",
            " * overflow\n",
            " * Trial\n",
            " * Maybe\n",
            " * Triathlon\n",
            " * trial\n",
            "\n",
            " {'interpreted': 1, 'language': 2, 'unique': 1, 'therefore': 1, 'maybe': 2, 'is': 2, 'an': 1, 'programming': 1, 'stack': 1, 'a': 1, 'generalpurpose': 1, 'Python': 2, 'highlevel': 1, 'overflow': 1, 'Trial': 2, 'Maybe': 1, 'Triathlon': 1, 'trial': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def uniqueWords():\n",
        "\n",
        "  unique_words = set()\n",
        "\n",
        "  # open file\n",
        "  with open('unique.txt') as text_file:\n",
        "    text = text_file.read()\n",
        "\n",
        "  # removing punctuations\n",
        "  words = lines = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "  for word in words:\n",
        "    # add words to set\n",
        "    unique_words.add(word)\n",
        "\n",
        "  print(f'The unique words are:')\n",
        "  for u_word in unique_words:\n",
        "    print(' *', u_word)\n",
        "\n",
        "  return None\n",
        "\n",
        "uniqueWords()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wpV62TS-2Ny",
        "outputId": "35b1f548-00bd-49ee-f219-16143cda5e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unique words are:\n",
            " * trial\n",
            " * interpreted\n",
            " * maybe\n",
            " * Triathlon\n",
            " * an\n",
            " * programming\n",
            " * language\n",
            " * stack\n",
            " * Python\n",
            " * Maybe\n",
            " * highlevel\n",
            " * therefore\n",
            " * Trial\n",
            " * overflow\n",
            " * is\n",
            " * generalpurpose\n",
            " * unique\n",
            " * a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wordFrequency():\n",
        "\n",
        "  word_freq = {}\n",
        "  unique_words = set()\n",
        "\n",
        "  with open('unique.txt') as text_file:\n",
        "    text = text_file.read()\n",
        "\n",
        "  # removing punctuations\n",
        "  words = lines = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "  # add words to set\n",
        "  for word in words:\n",
        "    unique_words.add(word)\n",
        "\n",
        "  for u_word in unique_words:\n",
        "    # count word frequency\n",
        "    n = words.count(u_word)\n",
        "    # add word and word frequency to dict.\n",
        "    word_freq.update({u_word:n})\n",
        "\n",
        "  print(word_freq)\n",
        "\n",
        "  return None\n",
        "\n",
        "wordFrequency()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uYd3vN2BpBA",
        "outputId": "89cee20d-0c64-4d03-e634-a969e74058e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'trial': 2, 'interpreted': 1, 'maybe': 2, 'Triathlon': 1, 'an': 1, 'programming': 1, 'language': 2, 'stack': 1, 'Python': 2, 'Maybe': 1, 'highlevel': 1, 'therefore': 1, 'Trial': 2, 'overflow': 1, 'is': 2, 'generalpurpose': 1, 'unique': 1, 'a': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIrZy4tVTXGw",
        "outputId": "7d296b81-43df-447c-cca7-5d8f703b5ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Python',\n",
              " 'is',\n",
              " 'an',\n",
              " 'interpreted',\n",
              " 'highlevel',\n",
              " 'generalpurpose',\n",
              " 'programming',\n",
              " 'language',\n",
              " 'Python',\n",
              " 'is',\n",
              " 'a',\n",
              " 'language',\n",
              " 'therefore',\n",
              " 'Trial',\n",
              " 'unique',\n",
              " 'stack',\n",
              " 'overflow',\n",
              " 'Trial',\n",
              " 'trial',\n",
              " 'trial',\n",
              " 'Triathlon',\n",
              " 'Maybe',\n",
              " 'maybe',\n",
              " 'maybe']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ71Q/vLE9BRSfLqkewNcR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}